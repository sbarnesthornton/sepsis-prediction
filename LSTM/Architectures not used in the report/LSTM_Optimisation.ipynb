{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VATcrXUrR7s8",
        "outputId": "d6299f27-db5f-4cea-ab42-6b8c2d30171f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLWxSLuJSBXj",
        "outputId": "816153d6-38e5-4f5d-c03a-cca34bb3d7ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.22.4)\n",
            "Collecting colorama>=0.4.6\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.2.0)\n",
            "Installing collected packages: colorama, bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.4.3 colorama-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!pip install bayesian-optimization\n",
        "from bayes_opt import BayesianOptimization\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset, Subset, WeightedRandomSampler\n",
        "from torch.nn.utils.rnn import pack_sequence\n",
        "from sklearn.metrics import f1_score,roc_auc_score\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.metrics import precision_recall_fscore_support as score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_EsiI1zSOzE"
      },
      "outputs": [],
      "source": [
        "class VitalSignsDataset(Dataset):\n",
        "    def __init__(self, slice_size, method):\n",
        "        self.method = method\n",
        "        if self.method == 'train':        \n",
        "          self.df = pd.read_csv('/content/drive/MyDrive/DataScience/project/no_additional_features.zip (Unzipped Files)/train_set_interpolation_with_multivariate.csv')\n",
        "        elif self.method =='test':\n",
        "          self.df = pd.read_csv('/content/drive/MyDrive/DataScience/project/no_additional_features.zip (Unzipped Files)/test_set_interpolation_with_multivariate.csv')\n",
        "        elif self.method =='val':\n",
        "          self.df = pd.read_csv('/content/drive/MyDrive/DataScience/project/no_additional_features.zip (Unzipped Files)/val_set_interpolation_with_multivariate.csv')\n",
        "\n",
        "        self.slice_size = slice_size\n",
        "        index_conversion = self.df[\n",
        "            self.df.groupby(\"Patient_ID\").cumcount(ascending=False) >= self.slice_size - 1\n",
        "        ]\n",
        "        index_conversion = index_conversion.reset_index()\n",
        "        self.idx_to_idx = index_conversion[[\"index\"]].to_numpy().reshape(-1)\n",
        "\n",
        "        # self.vitals = self.df.drop(\n",
        "        #     # [\"Patient_ID\", \"SepsisLabel\", \"ICULOS\", \"HospAdmTime\", \"Gender\", \"Age\"],\n",
        "        #     [\"SepsisLabel\", \"ICULOS\", \"HospAdmTime\", \"Gender\", \"Age\"],\n",
        "        #     axis=1,\n",
        "        # ).to_numpy(np.float32)\n",
        "        self.vitals = self.df.drop([\"SepsisLabel\"],axis=1).to_numpy(np.float32)\n",
        "        self.labels = self.df[[\"SepsisLabel\"]].to_numpy(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_idx)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        index = self.idx_to_idx[idx]\n",
        "        return (\n",
        "            self.vitals[(index) : (index + self.slice_size)],\n",
        "            self.labels[index + self.slice_size - 1],\n",
        "        )\n",
        "\n",
        "    def get_labels(self):\n",
        "        # x = [\n",
        "        #     ([0 for x in range(0, i)] + [1 for x in range(i, self.slice_size)])\n",
        "        #     for i in range(0, self.slice_size + 1)\n",
        "        # ]\n",
        "        # return [\"\".join([str(i) for i in l]) for l in x]\n",
        "        return [0, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6atL2seSQaD"
      },
      "outputs": [],
      "source": [
        "class TimeSeriesModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=40,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.fc1 = nn.Linear(40, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output, (h_n, c_n) = self.lstm(x)\n",
        "        output = self.fc1(output[:, -1])\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxpjrXKEYMXY"
      },
      "outputs": [],
      "source": [
        "def compute_prediction_utility(labels, predictions, dt_early=-12, dt_optimal=-6, dt_late=3.0, max_u_tp=1, min_u_fn=-2, u_fp=-0.05, u_tn=0):\n",
        "    # Check inputs for errors.\n",
        "    # Does the patient eventually have sepsis?\n",
        "    if np.any(labels):\n",
        "        is_septic = True\n",
        "        t_sepsis = np.argmax(labels) - dt_optimal \n",
        "    else:\n",
        "        is_septic = False\n",
        "        t_sepsis = float('inf')\n",
        "\n",
        "    n = len(labels)\n",
        "\n",
        "    # Define slopes and intercept points for utility functions of the form\n",
        "    # u = m * t + b.\n",
        "    m_1 = float(max_u_tp) / float(dt_optimal - dt_early)\n",
        "    b_1 = -m_1 * dt_early\n",
        "    m_2 = float(-max_u_tp) / float(dt_late - dt_optimal)\n",
        "    b_2 = -m_2 * dt_late\n",
        "    m_3 = float(min_u_fn) / float(dt_late - dt_optimal)\n",
        "    b_3 = -m_3 * dt_optimal\n",
        "\n",
        "    # Compare predicted and true conditions.\n",
        "    u = np.zeros(n)\n",
        "    for t in range(n):\n",
        "        if t <= t_sepsis + dt_late:\n",
        "            # TP\n",
        "            if is_septic and predictions[t]:\n",
        "                if t <= t_sepsis + dt_optimal:\n",
        "                    u[t] = max(m_1 * (t - t_sepsis) + b_1, u_fp)\n",
        "                elif t <= t_sepsis + dt_late:\n",
        "                    u[t] = m_2 * (t - t_sepsis) + b_2\n",
        "            # FP\n",
        "            elif not is_septic and predictions[t]:\n",
        "                u[t] = u_fp\n",
        "            # FN\n",
        "            elif is_septic and not predictions[t]:\n",
        "                if t <= t_sepsis + dt_optimal:\n",
        "                    u[t] = 0\n",
        "                elif t <= t_sepsis + dt_late:\n",
        "                    u[t] = m_3 * (t - t_sepsis) + b_3\n",
        "            # TN\n",
        "            elif not is_septic and not predictions[t]:\n",
        "                u[t] = u_tn\n",
        "\n",
        "    # Find total utility for patient.\n",
        "    return np.sum(u)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMz38yNhYHYs"
      },
      "outputs": [],
      "source": [
        "def Uscore(y_pred, y_actual, X_actual):\n",
        "  # Group Patients by ID\n",
        "  val_y_withID = y_actual.to_frame().join(X_actual['Patient_ID'])\n",
        "  grouped = val_y_withID.groupby('Patient_ID').groups\n",
        "  u_list=[]\n",
        "\n",
        "  # Initilalise Utilities\n",
        "  num_patients = val_y_withID['Patient_ID'].nunique()\n",
        "\n",
        "  observed_utilities = np.zeros(num_patients)\n",
        "  best_utilities = np.zeros(num_patients)\n",
        "  inaction_utilities = np.zeros(num_patients)\n",
        "\n",
        "  k = 0\n",
        "\n",
        "  for id, idx in grouped.items():\n",
        "    patient_actual = y_actual[idx[0]:idx[-1]+1]\n",
        "    patient_pred = y_pred[idx[0]:idx[-1]+1]\n",
        "\n",
        "    best_predictions = np.zeros(len(patient_actual))\n",
        "    inaction_predictions = np.zeros(len(patient_actual))\n",
        "\n",
        "    if np.any(patient_actual):\n",
        "      t_sepsis = np.argmax(patient_actual) + 6\n",
        "      best_predictions[max(0, t_sepsis - 12) : min(t_sepsis + 3 + 1, len(best_predictions))] = 1\n",
        "\n",
        "    \n",
        "    observed_utilities[k] = compute_prediction_utility(patient_actual,patient_pred)\n",
        "    best_utilities[k] = compute_prediction_utility(patient_actual,best_predictions)\n",
        "    inaction_utilities[k] = compute_prediction_utility(patient_actual, inaction_predictions)\n",
        "\n",
        "    k += 1 \n",
        "\n",
        "  unnormalized_observed_utility = np.sum(observed_utilities)\n",
        "  unnormalized_best_utility = np.sum(best_utilities)\n",
        "  unnormalized_inaction_utility = np.sum(inaction_utilities)\n",
        "\n",
        "  return (unnormalized_observed_utility - unnormalized_inaction_utility) / (unnormalized_best_utility - unnormalized_inaction_utility)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thl0HcvPS4ak"
      },
      "outputs": [],
      "source": [
        "# batch size 1024, epochs 50\n",
        "def rf_val(batch_size, epochs, Window_Size, hiddensize, learningrate):\n",
        "\n",
        "    batch_size = int(batch_size)\n",
        "    epochs= int(epochs)\n",
        "    Window_Size = int(Window_Size)\n",
        "    hiddensize = int(hiddensize)\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "    train_dataset = VitalSignsDataset(slice_size=Window_Size, method = \"train\")\n",
        "    test_dataset = VitalSignsDataset(slice_size=Window_Size, method = \"test\")\n",
        "    # val_dataset = VitalSignsDataset(slice_size=Window_Size, method = \"test\")\n",
        "\n",
        "    labels = [int(label[0]) for _, label in train_dataset]\n",
        "    # class_weights = {\"0\": class_Weights_nonspesis,\"1\":class_Weights_spesis}\n",
        "    class_weights = {\"0\": 0.023471489401092338,\"1\":0.9765285105989077}\n",
        "\n",
        "    sample_weights = [class_weights[str(label)] for label in labels]\n",
        "    sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1)\n",
        "    # val_loader = DataLoader(val_dataset, batch_size=1)\n",
        "\n",
        "    model = TimeSeriesModel(input_size=36,hidden_size= hiddensize)\n",
        "    model.to(device)\n",
        "    # print(model)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learningrate, betas=(0.9, 0.999))\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    n_epochs = 50\n",
        "    for epoch in range(n_epochs):\n",
        "        # print(f\"Epoch: {epoch}\")\n",
        "\n",
        "        model.train()\n",
        "        for i, (X_batch, y_batch) in enumerate(train_loader):\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            \n",
        "            y_pred = model(X_batch)\n",
        "            loss = loss_fn(y_pred, y_batch)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "    test_df= VitalSignsDataset(slice_size=1,method = \"test\")\n",
        "    # df = pd.read_csv('/content/drive/MyDrive/DataScience/project/no_additional_features.zip (Unzipped Files)/val_set_interpolation_with_multivariate.csv')\n",
        "    df = pd.read_csv('/content/drive/MyDrive/DataScience/project/no_additional_features.zip (Unzipped Files)/test_set_interpolation_with_multivariate.csv')\n",
        "    y_test = df.SepsisLabel\n",
        "    X_actual = df.drop('SepsisLabel',axis=1)\n",
        "    test_loader = DataLoader(test_df, batch_size=len(y_test))\n",
        "    inputs, classes = next(iter(test_loader))\n",
        "\n",
        "    model = model.to(device)\n",
        "    inputs=inputs.to(device)\n",
        "\n",
        "    y_pred=model(inputs)\n",
        "\n",
        "    y_pred = (y_pred.cpu().flatten() > torch.Tensor([0.5])).float()\n",
        "    \n",
        "    # return -(1-roc_auc_score(y_test, preds))\n",
        "    return -(1-Uscore(y_pred, y_test, X_actual))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeADZ2zpTBmb"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "HoRnugJVguyU",
        "outputId": "6948ec0b-dfb9-47d3-ea51-0ca7255255ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | Window... | batch_... |  epochs   | hidden... | learni... |\n",
            "-------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c851621cef44>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0mx_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constraint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-f410814d929a>\u001b[0m in \u001b[0;36mrf_val\u001b[0;34m(batch_size, epochs, Window_Size, hiddensize, learningrate)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# class_Weights_spesis, class_Weights_nonspesis,batch_size, epochs, Window_Size\n",
        "# set bounds for search\n",
        "pbounds = {\n",
        "        'batch_size': (12, 60),\n",
        "        'epochs': (48, 100),\n",
        "       'Window_Size': (10, 15),\n",
        "        'hiddensize' : (30,60),\n",
        "        'learningrate': (5e-6,1e-5)\n",
        "    }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=rf_val,\n",
        "    pbounds=pbounds,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "optimizer.maximize(init_points=10, n_iter=100)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}